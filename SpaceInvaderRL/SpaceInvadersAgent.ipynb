{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51713bba-75c2-4c80-ba22-f48e81bf439b",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859385b3-1a8c-4f82-a3ce-f31bdeffee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import atari_py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9c164-ce54-4a1c-a2c3-dfa5be9b1245",
   "metadata": {},
   "source": [
    "## Show the list of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ceabd4-6e45-4a03-876c-be38b739d21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'air_raid', 'alien', 'amidar', 'assault', 'asterix', 'asteroids', 'atlantis', 'bank_heist', 'battle_zone', 'beam_rider', 'berzerk', 'bowling', 'boxing', 'breakout', 'carnival', 'centipede', 'chopper_command', 'crazy_climber', 'defender', 'demon_attack', 'donkey_kong', 'double_dunk', 'elevator_action', 'enduro', 'fishing_derby', 'freeway', 'frogger', 'frostbite', 'galaxian', 'gopher', 'gravitar', 'hero', 'ice_hockey', 'jamesbond', 'journey_escape', 'kaboom', 'kangaroo', 'keystone_kapers', 'king_kong', 'koolaid', 'krull', 'kung_fu_master', 'laser_gates', 'lost_luggage', 'montezuma_revenge', 'mr_do', 'ms_pacman', 'name_this_game', 'pacman', 'phoenix', 'pitfall', 'pong', 'pooyan', 'private_eye', 'qbert', 'riverraid', 'road_runner', 'robotank', 'seaquest', 'sir_lancelot', 'skiing', 'solaris', 'space_invaders', 'star_gunner', 'surround', 'tennis', 'tetris', 'time_pilot', 'trondead', 'tutankham', 'up_n_down', 'venture', 'video_pinball', 'wizard_of_wor', 'yars_revenge', 'zaxxon']\n"
     ]
    }
   ],
   "source": [
    "print(atari_py.list_games())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea52df6-8ebf-434a-b67c-4ae054024fee",
   "metadata": {},
   "source": [
    "## Creating our environment as spaceInvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c6011f-aeca-4729-84c7-b96e777aab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5277d9-5fa2-4f4f-999e-926724432e28",
   "metadata": {},
   "source": [
    "## Creating episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb2ca8e-9260-49a4-b57b-991072d012fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Score:215.0\n",
      "Episode: 2\n",
      "Score:225.0\n",
      "Episode: 3\n",
      "Score:55.0\n",
      "Episode: 4\n",
      "Score:200.0\n",
      "Episode: 5\n",
      "Score:355.0\n",
      "Episode: 6\n",
      "Score:70.0\n",
      "Episode: 7\n",
      "Score:245.0\n",
      "Episode: 8\n",
      "Score:150.0\n",
      "Episode: 9\n",
      "Score:210.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 #epochs\n",
    "\n",
    "for episode in range(1,episodes):\n",
    "    state = env.reset() #Every time we iterate we reset state to its original poistion and restart our agent at the begining\n",
    "    done = False #wether our agent has completed the level\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() #use to visualise what action our agent is doing\n",
    "        state,reward,done,info = env.step(env.action_space.sample()) #the action that our agents will take within each frame\n",
    "        #In each frame our agent will take action,now we are just going to take a random action\n",
    "        #.sample() will do random action out of action_space(total 6 possible action)\n",
    "        \n",
    "        # state will be next state after taking this action\n",
    "        \n",
    "        score += reward #What current reward is within this while loop\n",
    "    \n",
    "    print(\"Episode: {}\\nScore:{}\".format(episode,score))\n",
    "    \n",
    "env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a07640-3e71-4800-988a-3d7e3ab8b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "# it states we can take 6 possible action in this environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58506b61-304e-4f93-97d5-e3674f8d963b",
   "metadata": {},
   "source": [
    "### Building our neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587049fe-e030-4ba6-ba92-b4f47d7f75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Conv2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7a11ab-a22e-4520-a0e0-7182db946cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height,width,channels,actions): \n",
    "    #height width and channnels are pixel for our screen\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(8,8),strides=(4,4),activation='relu',input_shape=(3,height,width,channels)))\n",
    "    model.add(Conv2D(64,(4,4),strides=(2,2),activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(actions,activation='linear'))\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e93fad11-cd8d-462b-845d-4fb7e92343cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prinitng heigh,width and channel of our model\n",
    "env.observation_space\n",
    "\n",
    "#(210, 160, 3) represents a window our model will learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbeba9a-5201-479e-ae0f-b7f9c11a5363",
   "metadata": {},
   "source": [
    "## Creating the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9daff98e-bdf6-45bf-be16-94a3296fa156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 action\n"
     ]
    }
   ],
   "source": [
    "height,width,channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(actions,\"action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d18fb3-4883-4df0-813c-0212b0e9e7a8",
   "metadata": {},
   "source": [
    "## Delete the model from the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744158b6-9684-4753-8939-ec2e6a4e892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model # if we get any error run these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25df5b5-8c0f-4afc-bbb4-99447f8a3c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               42467840  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 42,639,718\n",
      "Trainable params: 42,639,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(height,width,channels,actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87fdb517-5af0-4733-bd9b-6c47cc05e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improting keras-rl2 reinforcement learning agents\n",
    "from rl.agents import DQNAgent #it is going to create our deep q-networls\n",
    "from rl.memory import SequentialMemory #Dataset that our model will be learning from\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "#policy is the behaviour of our agent. Once we've created optimal policy we've created optimal agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8479e9d-0d53-4aa9-9e3c-3003adbdcf36",
   "metadata": {},
   "source": [
    "## Building reinfircement learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa329b0-10a4-47db-b7a7-8f13e4b98188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model,actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),attr='eps',value_max=1.,value_min=.1,value_test=.2,nb_steps=10000)\n",
    "    \n",
    "    \"\"\"Linear Annealing Policy computes a current threshold value and\n",
    "    transfers it to an inner policy which chooses the action. The threshold\n",
    "    value is following a linear function decreasing over time.\"\"\"\n",
    "    \n",
    "    \n",
    "    memory = SequentialMemory(limit=2000,window_length=3)\n",
    "    \n",
    "    #creatint the agent\n",
    "    dqn = DQNAgent(model=model,memory=memory,policy=policy,\n",
    "                  enable_dueling_network=True,dueling_type='avg',\n",
    "                  nb_actions = actions,nb_steps_warmup=1000)\n",
    "    \n",
    "    return dqn\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f938996-513a-42ce-8f46-660465da30e4",
   "metadata": {},
   "source": [
    "## Create a variable that stores build agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "941cab96-c780-4906-b82a-171874c2ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model,actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ead378-954a-436b-977b-051e0ad3d42e",
   "metadata": {},
   "source": [
    "## Compiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf74fa5a-e69f-4b39-8697-d898f141d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dqn.compile(Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072b0aa-9a22-41a2-a3ee-4b1bfb716be3",
   "metadata": {},
   "source": [
    "## Training our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8112116-359a-445d-bbdc-e2e77a20ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 4000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\envs\\practicalRL\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4000/10000 [===========>..................] - ETA: 1:46:36 - reward: 0.2000done, took 4270.032 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x181f6edf808>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env,nb_steps=4000,visualize=False,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115d8cd-991c-40b1-9e5a-29c0eb7ac2a9",
   "metadata": {},
   "source": [
    "## Saving our model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e2bf5c1-9661-4a8e-872e-e53ab1986464",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('models/dqn2.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2125b-fbe4-4416-97d1-782c7b62e641",
   "metadata": {},
   "source": [
    "### Visulising our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dcbcde1-c623-4d25-8908-961c8455d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 260.000, steps: 1137\n",
      "Episode 2: reward: 120.000, steps: 657\n",
      "Episode 3: reward: 30.000, steps: 341\n",
      "Episode 4: reward: 230.000, steps: 1111\n",
      "Episode 5: reward: 155.000, steps: 819\n",
      "Episode 6: reward: 545.000, steps: 1193\n",
      "Episode 7: reward: 125.000, steps: 701\n",
      "Episode 8: reward: 110.000, steps: 1002\n",
      "Episode 9: reward: 85.000, steps: 549\n",
      "Episode 10: reward: 140.000, steps: 690\n",
      "180.0\n"
     ]
    }
   ],
   "source": [
    "dqn.load_weights('models/dqn2.h5f')\n",
    "scores = dqn.test(env,nb_episodes=10,visualize = True)\n",
    "print(np.mean(scores.history['episode_reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ae951-a589-44f5-a57b-7903d7089510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:practicalRL]",
   "language": "python",
   "name": "conda-env-practicalRL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
