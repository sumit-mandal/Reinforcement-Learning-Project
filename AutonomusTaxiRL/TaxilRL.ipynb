{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f7c11f-996c-480d-8b1a-74a87fb600ce",
   "metadata": {},
   "source": [
    "`Note - For detailed documentation wse SpaceInvaders`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66885b0-8907-4d8c-a74c-7cd15e76c7d7",
   "metadata": {},
   "source": [
    "## Import required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1c31a2f-05a7-4e9c-8c02-135e8a9b785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85959c-e6e7-40cc-9b69-36cd6b6c8e57",
   "metadata": {},
   "source": [
    "## Creatinng the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c04113-c79f-44c6-8098-97f5c5df8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943391c6-032c-41fa-bd6d-21e903ec13e0",
   "metadata": {},
   "source": [
    "## Taking random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b106b87-a5f2-4c68-a19c-ce9c7d5b47f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode9\n",
      " score-794\n",
      "\n",
      "-1 reward\n",
      "339 state\n",
      "True done\n",
      "{'prob': 1.0, 'TimeLimit.truncated': True} info\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "\n",
    "for episode in range(1,episodes):\n",
    "    \n",
    "    state = env.reset() #Every time we iterate we reset state to its original poistion and restart our agent at the begining\n",
    "    done = False # whether our agent has completed the level\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() #for visualisation\n",
    "        state,reward,done,info = env.step(env.action_space.sample()) ##the action that our agents will take within each frame\n",
    "        #In each frame our agent will take action,now we are just going to take a random action\n",
    "        #.sample() will do random action out of action_space(total 6 possible action)\n",
    "        \n",
    "        # state will be next state after taking this action\n",
    "        \n",
    "        score += reward#What current reward is within this while loop\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print('Episode{}\\n score{}\\n'.format(episode,score))\n",
    "        print(reward,\"reward\")\n",
    "        print(state,\"state\")\n",
    "        print(done,\"done\")\n",
    "        print(info,\"info\")\n",
    "        \n",
    "env.close()\n",
    "\n",
    "\n",
    "#env.step()=  Run one timestep of the environment's dynamics. When end of\n",
    "#episode is reached, you are responsible for calling `reset()`\n",
    "#to reset this environment's state.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce28a53-b8c4-4349-91b6-a3090b528360",
   "metadata": {},
   "source": [
    "## Creating Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45af8467-99df-441c-8946-d6f0105b1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. actions agent can take : 6\n",
      "Total number of state our environment has: 500\n",
      "Created array of zeros with shape : (500, 6)\n",
      "Visualise q table:\n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "print(\"Total no. actions agent can take :\",actions)\n",
    "\n",
    "state = env.observation_space.n\n",
    "print(\"Total number of state our environment has:\",state)\n",
    "\n",
    "q_table = np.zeros((state,actions))\n",
    "print(\"Created array of zeros with shape :\",q_table.shape)\n",
    "print(\"Visualise q table:\\n\",q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598cd4b-f9e0-4231-8b6a-8ba5467eb03b",
   "metadata": {},
   "source": [
    "## Creating parameters for q-learnning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc6e31b-3501-43ae-859a-f3f57e94321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000 #No. of timmes we are going to re-iterate through our algorithm\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99 # High priority on current reward than future rewars\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01 # 1% probability that we take a random action by the end of our decay of exploration rate\n",
    "\n",
    "exploration_decay_rate = 0.001 # we want to decay our exploration_rate as we train our neural networks\n",
    "\n",
    "#creating the list where we store all our rewards\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e348858-9650-455c-8be8-09787d2cf906",
   "metadata": {},
   "source": [
    "## Q-learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20affc0-ec87-4c53-8c58-f0a19ad3691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Training over*******\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0 # this is the current reward we have per episode\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        #Coding Exploration vs exploitation trade-off\n",
    "        exploration_threshold = random.uniform(0,1)\n",
    "        if exploration_threshold > exploration_rate: #intially  this value will be false\n",
    "            action = np.argmax(q_table[state,:]) # Take the action which give maximum value depennding on state we are in\n",
    "        else:\n",
    "            action = env.action_space.sample() # take random actions\n",
    "            \n",
    "        \n",
    "        #taking our action\n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        \n",
    "        \n",
    "        #Updating q table or computing q-value\n",
    "        \n",
    "        \n",
    "        #New Q[s,a] = Q[s,a] + α([R(s,a) + γ(maxQ'(s',a') - Q(s,a))])\n",
    "        \n",
    "        q_table[state,action] = q_table[state,action]*(1-learning_rate) + learning_rate*(reward + discount_rate*np.max(q_table[new_state,:]))\n",
    "        \n",
    "        #note - this equation has slight variation from the equation mentioned above\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "        \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "                        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode) \n",
    "    # it decays the exploration rate\n",
    "    #so that we start taking actions from our q-table rather than taking random actions\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "print(\"*****Training over*******\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d18a27a-90ac-4518-afcd-7d208cf8c5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ -2.36613996,   1.44087409,  -2.17755678,  -0.8980715 ,\n",
       "          9.6220697 ,  -8.58988931],\n",
       "       [  4.77225736,   3.69182565,   2.05328962,   6.24382132,\n",
       "         14.11880599,  -2.13533785],\n",
       "       ...,\n",
       "       [ -1.23789808,   2.51208828,  -1.27013433,  -1.33135003,\n",
       "         -8.18309942,  -7.37379978],\n",
       "       [ -2.75595891,   1.77124471,  -2.61926953,  -2.72934177,\n",
       "        -10.09029138,  -9.05297012],\n",
       "       [  0.2212298 ,   0.59087738,  -0.41941   ,  17.3539126 ,\n",
       "         -2.72881   ,  -3.73944683]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2ffbe-850e-40cd-bccd-d93f4e50e3af",
   "metadata": {},
   "source": [
    "## Calculate and prinnt average reward per thousand episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16274ae7-1f1d-4eed-82ab-893bdc7fb575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per thousand Episodes\n",
      "1000  :  -255.34100000000015\n",
      "2000  :  -39.164000000000016\n",
      "3000  :  2.1339999999999937\n",
      "4000  :  5.631999999999976\n",
      "5000  :  6.8089999999999735\n",
      "6000  :  7.254999999999961\n",
      "7000  :  7.241999999999964\n",
      "8000  :  7.298999999999962\n",
      "9000  :  7.3119999999999665\n",
      "10000  :  7.515999999999963\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average per thousand Episodes\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \" : \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72ed9a15-8731-4d85-9886-9ed661eb2725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "***Failed :( *******\n"
     ]
    }
   ],
   "source": [
    "## Visulaising the agent\n",
    "\n",
    "import time\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"Episode is:\" + str(episode))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        env.render() #For visualisitaion\n",
    "        \n",
    "        time.sleep(0.4)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:]) # maximum value in q_table of a given state\n",
    "        \n",
    "        new_state,reward,done,info= env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"***Goal Reached*****\")\n",
    "                time.sleep(2)\n",
    "                clear_output(wait=True)\n",
    "            else:\n",
    "                print(\"***Failed :( *******\")\n",
    "                clear_output(wait=True)\n",
    "                time.sleep(2)\n",
    "                \n",
    "            break\n",
    "                \n",
    "        state = new_state\n",
    "        \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73f00c-b025-45d2-a159-0bd92387eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add TaxilRL.ipynb\n",
    "! git commit -m \"14:18/23-05-2021\"\n",
    "! git push origin main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:practicalRL]",
   "language": "python",
   "name": "conda-env-practicalRL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
