{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3a1f04-0a4e-4118-9a71-d7a8edc214a1",
   "metadata": {},
   "source": [
    "## Importing the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11827e2f-29d1-484e-8dd9-117606387e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Flatten,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY #Agent will move only right\n",
    "# from nes_py.wrappers import joypad_space\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import save_model\n",
    "from keras.models import load_model\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde14358-2f60-4480-a8a4-e76c8aaafa35",
   "metadata": {},
   "source": [
    "## Creating the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c269914-4aff-4a7d-ba26-843778e88281",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env,RIGHT_ONLY) #Actions will only move right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38abaf78-d5d1-4aad-82cc-746f589159bb",
   "metadata": {},
   "source": [
    "## Taking random actions in our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76cbc93-833c-4c0a-aaeb-8c3ecff994cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_reward = 0\n",
    "# done = True\n",
    "\n",
    "# for step in range(100000):\n",
    "#     env.render()\n",
    "#     if done:\n",
    "#         state = env.reset()\n",
    "#     state,reward,done,info = env.step(env.action_space.sample())\n",
    "# #     preprocess_state(state)\n",
    "#     print(info)\n",
    "#     print(state)\n",
    "# #     break\n",
    "    \n",
    "#     total_reward += reward\n",
    "#     clear_output(wait=True)\n",
    "    \n",
    "# env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a20713-5aca-425e-981c-000f9600974e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9f0b4b-c787-4b89-8076-b944e3266f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows Preprocess_state image\n",
    "# state = env.reset()\n",
    "# state = preprocess_state(state)\n",
    "# print(f\"array of preprocessed image is\\n {state}\")\n",
    "# print(f\"state  shape is {state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f6984-0d35-459c-ab91-46ede8dfa968",
   "metadata": {},
   "source": [
    "## Building class(Brain) for Mario Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd8ec04-a2a4-48e5-a115-cbf805eb3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size): #state_size is input layer and action_size is output_layer\n",
    "        #Create variables for our agent\n",
    "        self.state_space = state_size\n",
    "        self.action_space = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.gamma = 0.8 #discount factor- Priority to immediate reward than longterm reward\n",
    "        self.chosenAction = 0\n",
    "        \n",
    "        \n",
    "        #Creating exploration variable\n",
    "        self.epsilon = 1\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay_epsilon = 0.0001\n",
    "        \n",
    "        \n",
    "        #Building Neural Networks for agent\n",
    "        self.main_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.update_target_network() # it will simply set wait of our main_network to our target network\n",
    "        \n",
    "        \n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64,(4,4),strides=4,padding = 'same', input_shape=self.state_space))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(64,(4,4),strides = 2,padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(64,(3,3),strides = 1, padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(512,activation='relu'))\n",
    "        model.add(Dense(256,activation='relu'))\n",
    "        model.add(Dense(self.action_space,activation='linear')) #action_space = possible acton we can take in our environment\n",
    "\n",
    "        model.compile(loss='mse',optimizer = Adam())\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights()) \n",
    "        ## This method will simply set wait of our main_network to our target network\n",
    "\n",
    "    #Create a function that allows agent to act differently in different state\n",
    "\n",
    "    def act(self,state,onGround):\n",
    "        \n",
    "        if onGround < 83: #83 is any arbitray number\n",
    "            print(\"on Ground\") \n",
    "            #we'll only make predictions when value is less than 83\n",
    "            if random.uniform(0,1) < self.epsilon:\n",
    "                self.chosenAction = np.random.randint(self.action_space)\n",
    "                return self.chosenAction  #take the random action\n",
    "\n",
    "            Q_value = self.main_network.predict(state) #it will give prediction. Prediction means Q(s,a)\n",
    "            print(Q_value)\n",
    "            self.chosenAction = np.argmax(Q_value[0])\n",
    "            return self.chosenAction\n",
    "        else : \n",
    "            print(\"We are not on ground\")\n",
    "            return self.chosenAction\n",
    "        \n",
    "        # by doing this we are checking if we are on the ground and are below the y position of 83 then we can make an action\n",
    "        # Else we are just going to return the action that we computed in our previous action-prediction\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # Function to update epsilon and decay over time.\n",
    "\n",
    "    def update_epsilon(self,episode):\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_epsilon * episode)\n",
    "        \n",
    "\n",
    "    #Creating train function\n",
    "    def train(self,batch_size):\n",
    "        #Taking minibath from memory\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "\n",
    "        #Get variables from batch so we can find q-value\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            target = self.main_network.predict(state)\n",
    "#             print(target)\n",
    "\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = (reward + self.gamma*np.amax(self.target_network.predict(next_state))) # it removes oscillation from happening\n",
    "                # we are periodically updating weights for our target model\n",
    "\n",
    "            self.main_network.fit(state,target,epochs=1,verbose = 0)\n",
    "\n",
    "\n",
    "    #Storing state-action reward in our memory\n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    #Predict the action without epsilon greedy policy\n",
    "    def get_pred_act(self,state):\n",
    "        Q_values = self.main_network.predict(state)\n",
    "        return np.argmax(Q_values[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def load(self,name):\n",
    "        self.main_network = load_model(name)\n",
    "        self.target_network = load_model(name)\n",
    "        \n",
    "    def save(self,name):\n",
    "        save_model(self.main_network,name)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d1a27-a241-47ef-a49e-85779ab7fbfc",
   "metadata": {},
   "source": [
    "### `we create target network to avoid oscillation. So that we can improve accuracy and training`\n",
    "\n",
    "`To be explicit, the role of the model (self.main_network) is to do the actual predictions on what action to take, and the target model (self.target_model) tracks what action we want our model to take.`\n",
    "\n",
    "`Why not just have a single model that does both? After all, if something is predicting the action to take, shouldn’t it be implicitly determine what model we want our model to take? This is actually one of those “weird tricks” in deep learning that DeepMind developed to get convergence in the DQN algorithm. If you use a single model, it can (and often does) converge in simple environments (such as the CartPole). But, the reason it doesn’t converge in these more complex environments is because of how we’re training the model: because we’re training it “on the fly.”`\n",
    "\n",
    "`As a result, we are doing training at each time step and, if we used a single network, would also be essentially changing the “goal” at each time step. Think of how confusing that would be! That would be like if a teacher told you to go finish pg. 6 in your textbook and, by the time you finished half of it, she changed it to pg. 9, and by the time you finished half of that, she told you to do pg. 21! This, therefore, causes a lack of convergence by a lack of clear direction in which to employ the optimizer, i.e. the gradients are changing too rapidly for stable convergence. So, to compensate, we have a network that changes more slowly that tracks our eventual goal and one that is trying to achieve those.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee2d7db-45e4-4b73-8c09-c27cc972ead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space Box(0, 255, (240, 256, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "action_space = env.action_space.n\n",
    "state_space = (80,88,1)\n",
    "\n",
    "print(\"env.observation_space\",env.observation_space)\n",
    "\n",
    "## Grayscaling and preprocessing our image to make it computationally in expensive\n",
    "\n",
    "from PIL import  Image\n",
    "\n",
    "def preprocess_state(state):\n",
    "    image = Image.fromarray(state)\n",
    "    image = image.resize((88,80))\n",
    "    image = image.convert('L')\n",
    "#     image.show()\n",
    "    image = np.array(image)\n",
    "#     print(image)\n",
    "    \n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "#We need to convert size of env.observati on_space(which is 240, 256, 3) to \n",
    "# (80,88,1) so that it is not computationally expensive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43497ba-6dd7-4600-9e04-cedded772b10",
   "metadata": {},
   "source": [
    "## Creating the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086da0b8-1a84-4953-87cc-50b5601038e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000000\n",
    "num_timesteps = 400000 #amount of frames we'll be training on at each episodes\n",
    "batch_size = 64\n",
    "DEBUG_LENGTH = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec82cbe-ae36-4ecb-af45-ec48bc9f5aad",
   "metadata": {},
   "source": [
    "# creating our deep q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d325457-6667-4852-9eb1-ce44b0fcfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(state_space,action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d29be-5c22-46dc-ad3b-c9766b5200d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are not on ground\n",
      "Action is 0\n",
      "info of  is y_pos 103\n",
      "Episode is :1\n",
      ",Total Time Step:9\n",
      " Current Reward:1\n",
      " Epsilon is :1.0\n"
     ]
    }
   ],
   "source": [
    "print('Starting Training')\n",
    "\n",
    "stuck_buffer = deque(maxlen=DEBUG_LENGTH)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    Return = 0\n",
    "    done = False\n",
    "    time_step = 0\n",
    "    onGround = 79 #this is set to 79 because when the y_pos is 79 we are on the ground\n",
    "    # we are only on ground at first\n",
    "    \n",
    "    \n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1,80,88,1)\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        env.render()\n",
    "        time_step += 1\n",
    "        \n",
    "        if t>1 and stuck_buffer.count(stuck_buffer[-1]) > DEBUG_LENGTH - 50:\n",
    "            #If the count of the last value in a buffer is equal to the DEBUG_LENGTH then we have \n",
    "            #250 of the same x poition inside of our buffer\n",
    "            action = dqn.act(state,onGround = 79)\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            action = dqn.act(state,onGround)\n",
    "        \n",
    "        print(f\"Action is {str(action)}\")\n",
    "        \n",
    "        next_state,reward,done,info = env.step(action)\n",
    "        \n",
    "        print(f\"info of  is y_pos {info['y_pos']}\")\n",
    "        onGround = info['y_pos']\n",
    "        stuck_buffer.append(info['x_pos'])\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1,80,88,1)\n",
    "        \n",
    "        dqn.store_transition(state,action,reward,next_state,done) #Store the transition\n",
    "        state = next_state #Set the state to next_state\n",
    "        \n",
    "        Return += reward\n",
    "        print(f\"Episode is :{str(i)}\\n,Total Time Step:{str(time_step)}\\n Current Reward:{str(Return)}\\n Epsilon is :{str(dqn.epsilon)}\")\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        #If we have more data than batch_size then we can start training on it\n",
    "        if len(dqn.memory) > batch_size and i > 5: # also we'll start training when i is greater than 5\n",
    "            dqn.train(batch_size)\n",
    "           \n",
    "           \n",
    "    dqn.update_epsilon(i)\n",
    "    clear_output(wait=True)\n",
    "    dqn.update_target_network()\n",
    "    \n",
    "    #Save model\n",
    "    dqn.save('MarioRL.h5')\n",
    "    \n",
    "env.close()\n",
    "    \n",
    "           \n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819f8d1-effe-44b3-8756-11378fa4195f",
   "metadata": {},
   "source": [
    "` We can check whether we are on the ground by checking this y position`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e39fb0-d824-4f48-872a-38a9432f2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save('MarioRL.h5') #dqn is instance of DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c2c2c9-bb4a-462b-8258-56ed3480a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load('MarioRL.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ad457-c434-408d-b426-30c479b8ee44",
   "metadata": {},
   "source": [
    "# Visualising the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325610e4-7503-45bd-8d70-a8a1a13a3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    done = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1,80,88,1)\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = dqn.get_pred_act(state)\n",
    "        next_state,reward,done,info = env.step(action)\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1,80,88,1)\n",
    "        state = next_state\n",
    "        \n",
    "env.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715f8d9-9bf6-471e-8656-8dd9b9395360",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add MarioRL.ipynb\n",
    "! git commit -m \"22:57/25-05-2021\"\n",
    "! git push origin main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:practicalRL]",
   "language": "python",
   "name": "conda-env-practicalRL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
