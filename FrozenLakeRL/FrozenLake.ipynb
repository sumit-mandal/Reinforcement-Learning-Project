{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f7c11f-996c-480d-8b1a-74a87fb600ce",
   "metadata": {},
   "source": [
    "`Note - For detailed documentation wse SpaceInvaders`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66885b0-8907-4d8c-a74c-7cd15e76c7d7",
   "metadata": {},
   "source": [
    "## Import required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c31a2f-05a7-4e9c-8c02-135e8a9b785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85959c-e6e7-40cc-9b69-36cd6b6c8e57",
   "metadata": {},
   "source": [
    "## Creatinng the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c04113-c79f-44c6-8098-97f5c5df8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943391c6-032c-41fa-bd6d-21e903ec13e0",
   "metadata": {},
   "source": [
    "## Taking random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b106b87-a5f2-4c68-a19c-ce9c7d5b47f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode9\n",
      " score0.0\n",
      "\n",
      "0.0 reward\n",
      "5 state\n",
      "True done\n",
      "{'prob': 0.3333333333333333} info\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "\n",
    "for episode in range(1,episodes):\n",
    "    \n",
    "    state = env.reset() #Every time we iterate we reset state to its original poistion and restart our agent at the begining\n",
    "    done = False # whether our agent has completed the level\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() #for visualisation\n",
    "        state,reward,done,info = env.step(env.action_space.sample()) ##the action that our agents will take within each frame\n",
    "        #In each frame our agent will take action,now we are just going to take a random action\n",
    "        #.sample() will do random action out of action_space(total 5 possible action)\n",
    "        \n",
    "        # state will be next state after taking this action\n",
    "        \n",
    "        score += reward#What current reward is within this while loop\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print('Episode{}\\n score{}\\n'.format(episode,score))\n",
    "        print(reward,\"reward\")\n",
    "        print(state,\"state\")\n",
    "        print(done,\"done\")\n",
    "        print(info,\"info\")\n",
    "        \n",
    "env.close()\n",
    "\n",
    "\n",
    "#env.step()=  Run one timestep of the environment's dynamics. When end of\n",
    "#episode is reached, you are responsible for calling `reset()`\n",
    "#to reset this environment's state.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce28a53-b8c4-4349-91b6-a3090b528360",
   "metadata": {},
   "source": [
    "## Creating Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45af8467-99df-441c-8946-d6f0105b1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. actions agent can take : 4\n",
      "Total number of state our environment has: 16\n",
      "Created array of zeros with shape : (16, 4)\n",
      "Visualise q table:\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "print(\"Total no. actions agent can take :\",actions)\n",
    "\n",
    "state = env.observation_space.n\n",
    "print(\"Total number of state our environment has:\",state)\n",
    "\n",
    "q_table = np.zeros((state,actions))\n",
    "print(\"Created array of zeros with shape :\",q_table.shape)\n",
    "print(\"Visualise q table:\\n\",q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598cd4b-f9e0-4231-8b6a-8ba5467eb03b",
   "metadata": {},
   "source": [
    "## Creating parameters for q-learnning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc6e31b-3501-43ae-859a-f3f57e94321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000 #No. of timmes we are going to re-iterate through our algorithm\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99 # High priority on current reward than future rewars\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01 # 1% probability that we take a random action by the end of our decay of exploration rate\n",
    "\n",
    "exploration_decay_rate = 0.001 # we want to decay our exploration_rate as we train our neural networks\n",
    "\n",
    "#creating the list where we store all our rewards\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e348858-9650-455c-8be8-09787d2cf906",
   "metadata": {},
   "source": [
    "## Q-learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20affc0-ec87-4c53-8c58-f0a19ad3691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Training over*******\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0 # this is the current reward we have per episode\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        #Coding Exploration vs exploitation trade-off\n",
    "        exploration_threshold = random.uniform(0,1)\n",
    "        if exploration_threshold > exploration_rate: #intially  this value will be false\n",
    "            action = np.argmax(q_table[state,:]) # Take the action which give maximum value depennding on state we are in\n",
    "        else:\n",
    "            action = env.action_space.sample() # take random actions\n",
    "            \n",
    "        \n",
    "        #taking our action\n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        \n",
    "        \n",
    "        #Updating q table or computing q-value\n",
    "        \n",
    "        \n",
    "        #New Q[s,a] = Q[s,a] + α([R(s,a) + γ(maxQ'(s',a') - Q(s,a))])\n",
    "        \n",
    "        q_table[state,action] = q_table[state,action]*(1-learning_rate) + learning_rate*(reward + discount_rate*np.max(q_table[new_state,:]))\n",
    "        \n",
    "        #note - this equation has slight variation from the equation mentioned above\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "        \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "                        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode) \n",
    "    # it decays the exploration rate\n",
    "    #so that we start taking actions from our q-table rather than taking random actions\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "print(\"*****Training over*******\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d18a27a-90ac-4518-afcd-7d208cf8c5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54101808, 0.4827518 , 0.47506185, 0.48083297],\n",
       "       [0.30505589, 0.29231558, 0.26341336, 0.46606886],\n",
       "       [0.40565976, 0.39960215, 0.39219694, 0.42743327],\n",
       "       [0.36137333, 0.34883911, 0.27471928, 0.41432135],\n",
       "       [0.5731125 , 0.36956339, 0.42071479, 0.27391452],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.26649005, 0.16675625, 0.18851886, 0.14993957],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.36404505, 0.31179086, 0.31528235, 0.62047771],\n",
       "       [0.38409006, 0.65669199, 0.49898953, 0.40203865],\n",
       "       [0.57984979, 0.38307678, 0.29619692, 0.29224732],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.40749339, 0.62964478, 0.71458455, 0.49577822],\n",
       "       [0.68631928, 0.81062093, 0.68718409, 0.64358696],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2ffbe-850e-40cd-bccd-d93f4e50e3af",
   "metadata": {},
   "source": [
    "## Calculate and prinnt average reward per thousand episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16274ae7-1f1d-4eed-82ab-893bdc7fb575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per thousand Episodes\n",
      "1000  :  0.05200000000000004\n",
      "2000  :  0.20500000000000015\n",
      "3000  :  0.4040000000000003\n",
      "4000  :  0.5620000000000004\n",
      "5000  :  0.5990000000000004\n",
      "6000  :  0.6900000000000005\n",
      "7000  :  0.7050000000000005\n",
      "8000  :  0.6650000000000005\n",
      "9000  :  0.6910000000000005\n",
      "10000  :  0.6530000000000005\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average per thousand Episodes\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \" : \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ed9a15-8731-4d85-9886-9ed661eb2725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "***Goal Reached*****\n"
     ]
    }
   ],
   "source": [
    "## Visulaising the agent\n",
    "\n",
    "import time\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"Episode is:\" + str(episode))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        env.render() #For visualisitaion\n",
    "        \n",
    "        time.sleep(0.4)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:]) # maximum value in q_table of a given state\n",
    "        \n",
    "        new_state,reward,done,info= env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"***Goal Reached*****\")\n",
    "                time.sleep(2)\n",
    "                clear_output(wait=True)\n",
    "            else:\n",
    "                print(\"***Failed :( *******\")\n",
    "                clear_output(wait=True)\n",
    "                time.sleep(2)\n",
    "                \n",
    "            break\n",
    "                \n",
    "        state = new_state\n",
    "        \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73f00c-b025-45d2-a159-0bd92387eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add FrozenLake.ipynb\n",
    "! git commit -m \"14:15/23-05-2021\"\n",
    "! git push origin main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:practicalRL]",
   "language": "python",
   "name": "conda-env-practicalRL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
